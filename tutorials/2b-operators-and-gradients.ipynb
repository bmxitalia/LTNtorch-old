{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Complementary Notebook: Appropriate Operators to Approximate Connectives and Quantifiers\n",
    "\n",
    "This notebook is a complement to the tutorial on operators.\n",
    "\n",
    "Logical connectives in LTN are grounded using fuzzy semantics. However, while all fuzzy logic operators make sense when simply *querying* the language, not every operator is equally suited for *learning* in LTN. This notebook details the motivations behind some choice of operators over the others."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import ltn\n",
    "import torch"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Querying\n",
    "\n",
    "One can access the most common fuzzy logic operators in `ltn.fuzzy_ops`. They are implemented using PyTorch primitives.\n",
    "\n",
    "Here, we compare:\n",
    "- the product t-norm: $u \\land_{\\mathrm{prod}} v = uv$,\n",
    "- the Lukasiewicz t-norm: $u \\land_{\\mathrm{luk}} v = \\max(u+v-1,0)$,\n",
    "- the minimum aggregator: $\\min(u_1,\\dots,u_n)$,\n",
    "- the p-mean error aggregator (generalized mean of the deviations w.r.t. the truth): $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}}$.\n",
    "\n",
    "Each operator obviously conveys very different meanings, but they can all make sense depending on the intent of the query."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2800)\n",
      "tensor(0.1000)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(0.4)\n",
    "x2 = torch.tensor(0.7)\n",
    "\n",
    "# the stable keyword is explained at the end of the notebook\n",
    "and_prod = ltn.fuzzy_ops.AndProd(stable=False)\n",
    "and_luk = ltn.fuzzy_ops.AndLuk()\n",
    "\n",
    "print(and_prod(x1, x2))\n",
    "print(and_luk(x1, x2))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1000)\n",
      "tensor(0.3134)\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1])\n",
    "\n",
    "# the stable keyword is explained at the end of the notebook\n",
    "forall_min = ltn.fuzzy_ops.AggregMin()\n",
    "forall_pME = ltn.fuzzy_ops.AggregPMeanError(p=4, stable=False)\n",
    "\n",
    "print(forall_min(xs, dim=0))\n",
    "print(forall_pME(xs, dim=0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Learning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While all operators are suitable in a querying setting, this not the case in a learning setting. Indeed, many fuzzy logic operators have derivatives not suitable for gradient-based algorithms. For more details, read [van Krieken et al., *Analyzing Differentiable Fuzzy Logic Operators*, 2020](https://arxiv.org/abs/2002.06100).\n",
    "\n",
    "Here, we give simple illustrations of such gradient issues.\n",
    "\n",
    "#### 1. Vanishing Gradients\n",
    "\n",
    "Some operators have vanishing gradients on some part of their domains.\n",
    "\n",
    "e.g. in $u \\land_{\\mathrm{luk}} v = \\max(u+v-1,0)$, if $u+v-1 < 0$, the gradients vanish."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "[tensor(0.), tensor(0.)]\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor(0.3, requires_grad=True)\n",
    "x2 = torch.tensor(0.5, requires_grad=True)\n",
    "\n",
    "y = and_luk(x1, x2)\n",
    "y.backward()  # this is necessary to compute the gradients\n",
    "res = y.item()\n",
    "gradients = [v.grad for v in [x1, x2]]\n",
    "# print the result of the aggregation\n",
    "print(res)\n",
    "# print gradients of x1 and x2\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2. Single-Passing Gradients\n",
    "\n",
    "Some operators have gradients propagating to only one input at a time, meaning that all other inputs will not benefit from learning at this step.\n",
    "\n",
    "e.g. in $\\min(u_1,\\dots,u_n)$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10000000149011612\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1], requires_grad=True)\n",
    "\n",
    "y = forall_min(xs, dim=0)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print the result of the aggregation\n",
    "print(res)\n",
    "# print gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3. Exploding Gradients\n",
    "\n",
    "Some operators have exploding gradients on some part of their domains.\n",
    "\n",
    "e.g. in $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}}$, on the edge case where all inputs are $1.0$."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "tensor([nan, nan, nan])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "\n",
    "y = forall_pME(xs, dim=0, p=4)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print the result of the aggregation\n",
    "print(res)\n",
    "# print the gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Stable Product Configuration\n",
    "\n",
    "#### Product Configuration\n",
    "\n",
    "Our general recommendation is to use the following \"product configuration\":\n",
    "* not: the standard negation  $\\lnot u = 1-u$,\n",
    "* and: the product t-norm $u \\land v = uv$,\n",
    "* or: the product t-conorm (probabilistic sum) $u \\lor v = u+v-uv$,\n",
    "* implication: the Gougen strong implication (Reichenbach implication) $u \\rightarrow v = 1 - u + uv$,\n",
    "* existential quantification (\"exists\"): the generalized mean (p-mean) $\\mathrm{pM}(u_1,\\dots,u_n) = \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n u_i^p \\biggr)^{\\frac{1}{p}} \\qquad p \\geq 1$,\n",
    "* universal quantification (\"for all\"): the generalized mean of \"the deviations w.r.t. the truth\" (p-mean error) $\\mathrm{pME}(u_1,\\dots,u_n) = 1 - \\biggl( \\frac{1}{n} \\sum\\limits_{i=1}^n (1-u_i)^p \\biggr)^{\\frac{1}{p}} \\qquad p \\geq 1$.\n",
    "\n",
    "#### \"Stable\"\n",
    "\n",
    "As is, this configuration is not fully exempt from issues. The product t-norm has vanishing gradients on the edge case $u=v=0$. The product t-conorm has vanishing gradients on the edge case $u=v=1$. The Reichenbach implication has vanishing gradients on the edge case $u=0$,$v=1$. p-mean has exploding gradients on the edge case $u_1=\\dots=u_n=0$. p-mean error has exploding gradients on the edge case $u_1=\\dots=u_n=1$.\n",
    "However, all these issues happen on edge cases and can easily be fixed using the following \"trick\":\n",
    "- if the edge case happens when an input $u$ is $0$, we modify every input with $u' = (1-\\epsilon)u+\\epsilon$,\n",
    "- if the edge case happens when an input $u$ is $1$, we modify every input with $u' = (1-\\epsilon)u$,\n",
    "\n",
    "where $\\epsilon$ is a small positive value (e.g. $1\\mathrm{e}{-5}$).\n",
    "\n",
    "This gives us stabilized versions of such operators. One can trigger the stable behavior by using the boolean keyword `stable`. One can set a default value for `stable` when initializing the operator, or can use different values at each call of the operator."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9998999834060669\n",
      "tensor([0.3333, 0.3333, 0.3333])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1.], requires_grad=True)\n",
    "\n",
    "# the exploding gradient problem is solved\n",
    "y = forall_pME(xs, dim=0, p=4, stable=True)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print the result of the aggregation\n",
    "print(res)\n",
    "# print the gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### The hyper-parameter $p$ in the generalized means\n",
    "\n",
    "$p$ offers flexibility in writing more or less strict formulas, to account for outliers in the data depending on the application. Note that this can have strong implications for training."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31339913606643677\n",
      "tensor([0.0000, 0.0000, 0.0000, 0.0483, 0.1325, 0.1977, 0.1977, 0.2815])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1], requires_grad=True)\n",
    "\n",
    "y = forall_pME(xs, dim=0, p=4)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print result of aggregation\n",
    "print(res)\n",
    "# print gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18157517910003662\n",
      "tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0734e-05, 6.4147e-03, 8.1100e-02,\n",
      "        8.1100e-02, 7.6019e-01])\n"
     ]
    }
   ],
   "source": [
    "xs = torch.tensor([1., 1., 1., 0.5, 0.3, 0.2, 0.2, 0.1], requires_grad=True)\n",
    "\n",
    "y = forall_pME(xs, dim=0, p=20)\n",
    "res = y.item()\n",
    "y.backward()\n",
    "gradients = xs.grad\n",
    "# print result of aggregation\n",
    "print(res)\n",
    "# print gradients of xs\n",
    "print(gradients)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "While it can be tempting to set a high value for $p$ when querying, in a learning setting, this can quickly lead to a \"single-passing\" operator that will focus too much on outliers at each step (i.e., gradients overfitting one input at this step, potentially harming the training of the others). We recommend not to set a too high $p$ when learning.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}