{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Grounding in Logic Tensor Networks (LTN)\n",
    "\n",
    "## Real Logic\n",
    "\n",
    "The semantics of LTN differs from the standard abstract semantics of First-order Logic (FOL) in that domains are interpreted concretely by tensors in the Real field.\n",
    "To emphasize the fact that LTN interprets symbols which are grounded on real-valued features, we use the term *grounding*, denoted by $\\mathcal{G}$, instead of interpretation.\n",
    "$\\mathcal{G}$ associates a tensor of real numbers to any term of the language, and a real number in the interval $[0,1]$ to any formula $\\phi$.\n",
    "In the rest of the tutorials, we commonly use \"tensor\" to designate \"tensor in the Real field\".\n",
    "\n",
    "The language consists of a non-logical part (the signature) and logical connectives and quantifiers.\n",
    "* **constants** denote individuals from some space of tensors $\\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$ (tensor of any rank). The individual can be pre-defined (data point) or learnable (embedding).\n",
    "* **variables** denote sequences of individuals.\n",
    "* **functions** can be any mathematical function either pre-defined or learnable. Examples of functions can be distance functions, regressors, etc. Functions can be defined using any operations in PyTorch. They can be linear functions, Deep Neural Networks, and so forth.\n",
    "* **predicates** are represented as mathematical functions that map from some n-ary domain of individuals to a real number in $[0,1]$ that can be interpreted as a truth degree. Examples of predicates can be similarity measures, classifiers, etc.\n",
    "* **connectives** -- not, and, or, implies -- are modeled using fuzzy semantics.\n",
    "* **quantifiers** are defined using aggregators.\n",
    "\n",
    "This tutorial explains how to ground constants, variables, functions and predicates."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "import ltn\n",
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Grounding\n",
    "\n",
    "In LTN, a special class called `Grounding` is used to represent the grounding of LTN terms and formulas. As we have already said, grounding means\n",
    "converting a logical symbol (e.g., the constant \"john\") into a tensor in the Real field (e.g., vector of features corresponding to the demographic information of \"john\").\n",
    "So, every LTN term (i.e., constants, variables, functions) or formula (i.e, predicate, complex formula) is characterized by a `Grounding` object. An LTN `Grounding` object has two important\n",
    "attributes that are often used inside the core of LTN:\n",
    "1. `tensor` attribute: it contains the tensor representation (grounding) of the LTN term or formula. Specifically, LTNtorch\n",
    "uses PyTorch tensors to represent the grounding of an LTN term or formula;\n",
    "2. `free_variables` attribute: it is a list of strings containing the names (labels) of the free variables contained in the grounding of\n",
    "the LTN term or formula. Note that:\n",
    "    - a constant has not free variables, so the `free_variables` attribute for a constant will be an empty list;\n",
    "    - a variable has only one free variable, that is the variable itself;\n",
    "    - a function has a number of free variables that depends on the number of variables in input to the function;\n",
    "    - a predicate has a number of free variables that depends on the number of variables in input to the predicate;\n",
    "    - a formula has a number of free variables that depends on the number of variables that appears on the predicates that\n",
    "    define the formula and on the types of quantifications that are used in the formula.\n",
    "\n",
    "Examples of the use of these two important attributes are included in this notebook.\n",
    "\n",
    "## Constants\n",
    "\n",
    "LTN constants are grounded into real tensors. Each constant $c$ is mapped to a point in $\\mathcal{G}(c) \\in \\bigcup\\limits_{n_1 \\dots n_d \\in \\mathbb{N}^*} \\mathbb{R}^{n_1 \\times \\dots \\times n_d}$. Notice that the objects in the domain may be tensors of any rank. A tensor of rank $0$ corresponds to a scalar, a tensor of rank $1$ to a vector, a tensor of rank $2$ to a matrix and so forth, in the usual way.\n",
    "Here we define $\\mathcal{G}(c_1)=(2.1,3)$ and $\\mathcal{G}(c_2)=\\begin{pmatrix}\n",
    "4.2 & 3 & 2.5\\\\\n",
    "4 & -1.3 & 1.8\n",
    "\\end{pmatrix}$.\n",
    "\n",
    "The type of an LTN constant is `Grouding`, as we said before.\n",
    "In order to print the value (tensor) of an LTN constant, it is enough to invoke `print()` on the constant itself."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'ltn.core.Grounding'>\n",
      "tensor([2.1000, 3.0000])\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.constant([2.1, 3])\n",
    "c2 = ltn.constant([[4.2, 3, 2.5], [4, -1.3, 1.8]])\n",
    "print(type(c1))\n",
    "print(c1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Note that a constant can be set as learnable by using the optional argument `trainable=True`. This is useful to learn embeddings for some individuals.\n",
    "The features of the tensor will be considered as trainable parameters (learning in LTN is explained in a further notebook)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "c3 = ltn.constant([0.,0.], trainable=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "LTN constants are implemented using PyTorch tensors encapsulated into objects of type `Grounding`. If the parameter `trainable` of the LTN constant is set to `True`, then\n",
    "the `requires_grad` parameter of the PyTorch tensor inside the `Grounding` representing the LTN constant will be set to `True`. Since the constants are PyTorch tensors,\n",
    "it is possible to query the value of the constants easily.\n",
    "\n",
    "When it is needed to use the PyTorch tensor incapsulated inside an object of type `Grounding` it is possible to access the `tensor` attribute."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.1000, 3.0000])\n",
      "[2.1 3. ]\n",
      "tensor([0., 0.], requires_grad=True)\n",
      "[0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(c1)\n",
    "# to access to the tensor contained in the Grounding of the constant, we need to access to the `tensor` attribute\n",
    "print(c1.tensor.numpy())\n",
    "print(c3)\n",
    "# here, we have to perform a detach before calling numpy(), because the tensor has requires_grad=True\n",
    "print(c3.tensor.detach().numpy())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Predicates\n",
    "\n",
    "LTN predicates are grounded into mappings that assign a value between zero and one to some n-ary space of input values. Predicates in LTN can be neural networks or any other function that achieves such a mapping.\n",
    "Note that the input of an LTN predicate is a Grounding or a list of Grounding, and the output is a Grounding.\n",
    "\n",
    "There are different ways to construct a predicate in LTN. The constructor `ltn.Predicate(model, layers_size, lambda_func)` permits three types of construction:\n",
    "- if the `model` parameter is not `None`, the predicate is constructed by using the `torch.nn.Module` model instance that has been given as input;\n",
    "- if the `model` parameter is `None` and the `layers_size` parameter is not `None`, the predicate is constructed using a fully-connected MLP as the predicate model. A `torch.nn.Sequential` model is constructed\n",
    "using the size of the layers provided in the `layers_size` parameter, which has to be a tuple of integers. Note that the tuple must contain the size of the input and output layer too. An ELU activation is used on the hidden layers, and a sigmoid\n",
    "activation is used on the final layer;\n",
    "- if the `model` parameter is `None`, the `layers_size` parameter is `None`, and the `lambda_func` parameter is not `None`, the predicate is constructed by using the lambda function given in input. The construction by using\n",
    "the lambda function is suggested for small mathematical operations with **no weight tracking** (non-trainable function).\n",
    "\n",
    "The following defines a predicate $P_1$ using the lambda_func parameter, a predicate $P_2$ using the `model` parameter, and a predicate\n",
    "$P_3$ using the `layers_size` parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "mu = ltn.constant([2., 3.])\n",
    "# note the access to the `tensor` attribute since we need an operation between tensors\n",
    "P1 = ltn.Predicate(lambda_func=lambda x: torch.exp(-torch.norm(x[0] - mu.tensor, dim=1)))\n",
    "\n",
    "class ModelP2(torch.nn.Module):\n",
    "    \"\"\"For more info on how to use torch.nn.Module:\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.Module.html\"\"\"\n",
    "    def __init__(self):\n",
    "        super(ModelP2, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.dense1 = torch.nn.Linear(2, 5)\n",
    "        self.dense2 = torch.nn.Linear(5, 1) # returns one value in [0,1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[0]  # here, we have to access to x[0] because the input is always contained in a list due to LTN pre-processing\n",
    "        x = self.elu(self.dense1(x))\n",
    "        return self.sigmoid(self.dense2(x))\n",
    "\n",
    "modelP2 = ModelP2()\n",
    "P2 = ltn.Predicate(model=modelP2)\n",
    "\n",
    "P3 = ltn.Predicate(layers_size=(2, 5, 1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can easily query predicates using LTN constants and LTN variables (see further in this notebook to query using variables)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9048)\n",
      "tensor(0.0358)\n",
      "tensor(0.3226, grad_fn=<ViewBackward>)\n",
      "tensor(0.3385, grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.constant([2.1, 3])\n",
    "c2 = ltn.constant([4.5, 0.8])\n",
    "c3 = ltn.constant([3.0, 4.8])\n",
    "print(P1(c1))\n",
    "print(P1(c2))\n",
    "print(P2(c3))\n",
    "print(P3(c3))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "NOTE:\n",
    "- If an LTN predicate (or an LTN function) takes several inputs, e.g. $P_4(x_1,x_2)$, the arguments must be passed as a list.\n",
    "- LTN converts inputs such that there is a \"batch\" dimension on the first dim. Therefore, most operations should work with `dim=1`."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6034, grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "class ModelP4(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ModelP4, self).__init__()\n",
    "        self.elu = torch.nn.ELU()\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.dense1 = torch.nn.Linear(4, 5)\n",
    "        self.dense2 = torch.nn.Linear(5, 1) # returns one value in [0,1]\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.cat(x, dim=1)  # most of the operations will work with dim=1\n",
    "        x = self.elu(self.dense1(x))\n",
    "        return self.sigmoid(self.dense2(x))\n",
    "\n",
    "P4 = ltn.Predicate(ModelP4())\n",
    "c1 = ltn.constant([2.1, 3])\n",
    "c2 = ltn.constant([4.5, 0.8])\n",
    "print(P4([c1, c2])) # multiple arguments are passed as a list"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One may define trainable or non-trainable 0-ary predicates (propositional variables) using `ltn.propositional_variable`.\n",
    "They are grounded as a mathematical constant in $[0,1]$. Notice that if you decide to set the propositional variable as\n",
    "trainable, you should constraint its value in the range $[0,1]$ during learning. To do that, it is possible to use\n",
    "`torch.clamp()`. To understand how this works, it is possible to go through the `propositional_variables.py` example included\n",
    "in the repository. Note that LTNtorch will not constraint the value of a trainable propositional variable automatically.\n",
    "This is why a warning is printed when a trainable propositional variable is declared."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ciomi/PycharmProjects/LTNtorch/ltn/core.py:169: UserWarning: Attention! You have defined a trainable LTN propositional variable. Therefore, you should constraint its value in [0., 1.] during learning. LTNtorch does not do that automatically. It is possible to use torch.clamp() to do that. See the propositional_variables.py example to understand how this works.\n",
      "  warnings.warn(\"Attention! You have defined a trainable LTN propositional variable. Therefore, you should \"\n"
     ]
    }
   ],
   "source": [
    "# Declaring a trainable 0-ary predicate with initial truth value 0.3\n",
    "A = ltn.propositional_variable(0.3, trainable=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For more details and useful ways to create predicates (incl. how to integrate multiclass classifiers as binary predicates) see the complementary notebook."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Functions\n",
    "\n",
    "LTN functions are grounded as any mathematical function that maps $n$ individuals to one individual in the tensor domains.\n",
    "Note that the input of an LTN function is a Grounding or a list of Grounding, and the output is a Grounding.\n",
    "\n",
    "There are different ways to construct a function in LTN. The constructor `ltn.Function(model, layers_size, lambda_func)` permits three types of construction:\n",
    "- if the `model` parameter is not `None`, the function is constructed by using the `torch.nn.Module` model instance that has been given as input;\n",
    "- if the `model` parameter is `None` and the `layers_size` parameter is not `None`, the function is constructed using a fully-connected MLP as the function model. A `torch.nn.Sequential` model is constructed\n",
    "using the size of the layers provided in the `layers_size` parameter, which has to be a tuple of integers. Note that the tuple must contain the size of the input and output layer too. An ELU activation is used on the hidden layers,\n",
    "and a linear activation is used on the final layer;\n",
    "- if the `model` parameter is `None`, the `layers_size` parameter is `None`, and the `lambda_func` parameter is not `None`, the function is constructed by using the lambda function given in input. The construction by using\n",
    "a lambda function is suggested for small mathematical operations with **no weight tracking** (non-trainable function).\n",
    "\n",
    "The following defines a function $f_1$ using the lambda_func parameter, a function $f_2$ using the `model` parameter, and a function\n",
    "$f_3$ using the `layers_size` parameter."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "f1 = ltn.Function(lambda_func=lambda args: args[0] - args[1])\n",
    "\n",
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.dense1 = torch.nn.Linear(2, 10)\n",
    "        self.dense2 = torch.nn.Linear(10, 5)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x[0]\n",
    "        x = self.relu(self.dense1(x))\n",
    "        return self.dense2(x)\n",
    "\n",
    "model_f2 = MyModel()\n",
    "f2 = ltn.Function(model=model_f2)\n",
    "\n",
    "f3 = ltn.Function(layers_size=(2, 10, 5))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "One can easily compute functions using LTN constants and LTN variables as inputs (see further in this notebook to query using variables)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.4000,  2.2000])\n",
      "tensor([ 0.3361, -0.0311, -0.1417,  0.7412, -1.2933], grad_fn=<ViewBackward>)\n",
      "tensor([ 0.4747,  0.3733,  0.2163, -0.8041,  1.3979], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.constant([2.1, 3])\n",
    "c2 = ltn.constant([4.5, 0.8])\n",
    "print(f1([c1, c2])) # multiple arguments are passed as a list\n",
    "print(f2(c1))\n",
    "print(f3(c1))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Variables\n",
    "\n",
    "LTN variables are sequences of individuals/constants from a domain. Variables are useful to write quantified statements, e.g. $\\forall x\\ P(x)$. Notice that a variable is a sequence and not a set, namely, the same value can occur twice in the sequence.\n",
    "\n",
    "The following defines two variables, $x$ and $y$, with respectively 10 and 5 individuals sampled from normal distributions in $\\mathbb{R}^2$.\n",
    "In LTN, variables need to be labelled (see the arguments `'x'` and `'y'` below). These two labels will become the free variables of the variables.\n",
    "\n",
    "As for LTN constants, LTN variables are represented by a `Grounding` object. The example shows how to print the value of\n",
    "a variable, and the list of its free variables."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5049,  1.7585],\n",
      "        [ 0.7468, -0.3156],\n",
      "        [-0.2099, -0.7625],\n",
      "        [ 1.2200, -0.7437],\n",
      "        [-0.3670,  0.0066],\n",
      "        [-0.1341,  0.0603],\n",
      "        [-0.5561,  0.7926],\n",
      "        [ 0.4629,  0.6626],\n",
      "        [-0.7308,  0.0503],\n",
      "        [-1.3226, -0.2893]])\n",
      "['x']\n"
     ]
    }
   ],
   "source": [
    "x = ltn.variable('x', np.random.normal(0., 1., (10, 2)))\n",
    "y = ltn.variable('y', np.random.normal(0., 4., (5, 2)))\n",
    "print(x) # print the value of the variable (the tensor contained in the Grounding of the variable)\n",
    "print(x.free_variables) # print the list of free variables associated to the variable. Since it is a variable, the list of free variables contains only the variable itself"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluating a term/predicate on one variable of $n$ individuals yields $n$ output values, where the $i$-th output value corresponds to the term calculated with the $i$-th individual.\n",
    "\n",
    "Similarly, evaluating a term/predicate on $k$ variables $(x_1,\\dots,x_k)$, with respectively $n_1,\\dots,n_k$ individuals each, yields a result with $n_1 \\times \\dots \\times n_k$ values. The result is organized in a tensor (inside a Grounding object) where the first $k$ dimensions can be indexed to retrieve the outcome(s) that correspond to each variable. By accessing the `free_variables` attribute of the `Grounding`, it is possible to check which axis of the `tensor` attribute corresponds to which variable."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n",
      "['x', 'y']\n",
      "tensor(0.2891, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Notice that the outcome is a 2-dimensional tensor where each cell\n",
    "# represents the satisfiability of P4 evaluated with each individual in x and in y.\n",
    "P4 = ltn.Predicate(ModelP4())\n",
    "res1 = P4([x, y])\n",
    "# LTN predicates return Grounding objects. If we need to do operations on the tensors contained in these Grounding objects, we need to access to the `tensor` attribute.\n",
    "print(res1.tensor.shape)\n",
    "print(res1.free_variables) # as expected, the free_variables attribute of the result contains x and y, which are the free variables in the formula\n",
    "# free_variables is useful to understand which axis of the result is related to which variable. In this case, axis 0 is related to x, and axis 1 to y\n",
    "print(res1.tensor[2, 0]) # gives the result computed with the 3rd individual in x and the 1st individual in y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5, 2])\n",
      "['x', 'y']\n",
      "tensor([-3.0148,  2.4038])\n"
     ]
    }
   ],
   "source": [
    "# Notice that the last axe(s) correspond to the dimensions of the outcome;\n",
    "# here, f2 projects to outcomes in R^2, so the outcome has one additional axis of dimension 2.\n",
    "# the output tensor has shape (10, 5, 2) because variable x has 10 individuals, y has 5 individuals, and f1 maps in R^2\n",
    "res2 = f1([x, y])\n",
    "print(res2.tensor.shape)\n",
    "print(res2.free_variables)\n",
    "print(res2.tensor[2,0]) # gives the result calculated with the 3rd individual in x and the 1st individual in y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5])\n",
      "['y']\n",
      "tensor(0.2110, grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.constant([2.1, 3])\n",
    "res3 = P4([c1, y])\n",
    "print(res3.tensor.shape) # Notice that no axis is associated to a constant. The output has shape (5,) because variable y has\n",
    "# 5 individuals\n",
    "print(res3.free_variables)\n",
    "print(res3.tensor[0]) # gives the result calculated with c1 and the 1st individual in y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Variables made of trainable constants\n",
    "\n",
    "A variable can be instantiated using two different types of objects:\n",
    "- A value (numpy, python list, ...) that will be fed in a `torch.Tensor` (the variable refers to a new object).\n",
    "- A `torch.Tensor` instance that will be used directly as the variable (the variable refers to the same object).\n",
    "\n",
    "The latter is useful when the variable denotes a sequence of trainable constants. In order to better understand this\n",
    "topic, it is kindly suggested to carefully read the `Smokes-friends-cancer` example, where variables\n",
    "made of trainable constants are used."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.4177, 0.2967], grad_fn=<ViewBackward>)\n"
     ]
    }
   ],
   "source": [
    "c1 = ltn.constant([2.1, 3], trainable=True)\n",
    "c2 = ltn.constant([4.5, 0.8], trainable=True)\n",
    "\n",
    "# PyTorch will keep track of the gradients between c1, c2 and x.\n",
    "# Read tutorial 3 for more details.\n",
    "# notice the access to the tensor attribute of the Grounding objects. This is needed since torch.stack() requires tensors.\n",
    "x = ltn.variable('x', torch.stack([c1.tensor,c2.tensor]))\n",
    "res = P2(x)\n",
    "print(res)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}